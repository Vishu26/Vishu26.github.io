<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Srikumar Sastry</title>

    <meta name="author" content="Srikumar Sastry">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Srikumar Sastry
                </p>
                <p>I am a PhD candidate at <a href="https://washu.edu/">Washington University</a> in Imaging Science, working in <a href="https://mvrl.cse.wustl.edu/">Multimodal Vision Research Laboratory</a> led by <a href="https://jacobsn.github.io/">Dr. Nathan Jacobs</a>.
                </p>
                <p>
                  I have an MS in Geoinformatics from <a href="https://itc.nl/">The Faculty ITC, Geoinformation Science and Earth Observation</a>. During my Masters, I was supervised by <a href="https://research.utwente.nl/en/persons/mariana-belgiu">Dr. Mariana Belgiu</a> and <a href="https://research.utwente.nl/en/persons/raian-vargas-maretto">Dr. Raian Vargas Maretto</a>, and worked on developing active learning methods in remote sensing.
                </p>
                <p style="text-align:center">
                  <a href="mailto:srikumarsastry1997@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/SS_Resume_2025.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=A-nQa6EAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/Vishu26/">Github</a> &nbsp;/&nbsp;
                  <a href="#reviewer">Academic Service</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/sri.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/sri.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I am interested in multimodal learning, computer vision, remote sensing and generative/bayesian modeling. My work involves multimodal representation learning and fusing heterogeneous multimodal data for remote sensing applications involving fine-grained classification, satellite image synthesis, species distribution modeling, change detection, and land cover mapping.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='psm_image'>
                  <source src="images/tiny_framework_ultimate_2.png"></div>
                  <img src='images/tiny_framework_ultimate_2.png' width=95%>
                </div> 
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="">
                  <span class="papertitle">RANGE: Retrieval Augmented Neural Fields for Multi-Resolution Geo-Embeddings</span>
                </a>
                <br>
                <a href="https://sites.wustl.edu/aayush/">Aayush Dhakal</a>,
                <strong>Srikumar Sastry</strong>,
                <a href="https://subash-khanal.github.io/">Subash Khanal</a>,
                <a href="https://ericx003.github.io/">Eric Xing</a>,
                <a href="https://adealgis.wixsite.com/adeel-ahmad-geog">Adeel Ahmad</a>,
                <a href="https://jacobsn.github.io/">Nathan Jacobs</a>
                <br>
                <em>CVPR</em>, 2025
                <br>
                <a href="">project page</a>
                /
                <a href="">github</a>
                /
                <a href="">arXiv</a>
                <p></p>
                <p>
                  We propose a novel retrieval-augmented strategy called RANGE, to fully capture the important visual features necessary to represent a geographic location.
                </p>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='psm_image'>
                  <source src="taxabind/static/images/taxabind_logo.png"></div>
                  <img src='taxabind/static/images/taxabind_logo.png' width=95%>
                </div> 
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="taxabind/index.html">
                  <span class="papertitle">TaxaBind: A Unified Embedding Space for Ecological Applications</span>
                </a>
                <br>
                <strong>Srikumar Sastry</strong>,
                <a href="https://subash-khanal.github.io/">Subash Khanal</a>,
                <a href="https://sites.wustl.edu/aayush/">Aayush Dhakal</a>,
                <a href="https://adealgis.wixsite.com/adeel-ahmad-geog">Adeel Ahmad</a>,
                <a href="https://jacobsn.github.io/">Nathan Jacobs</a>
                <br>
                <em>WACV</em>, 2025 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                <br>
                <a href="taxabind/index.html">project page</a>
                /
                <a href="https://github.com/mvrl/TaxaBind">github</a>
                /
                <a href="https://arxiv.org/abs/2411.00683">arXiv</a>
                <p></p>
                <p>
                  TaxaBind is a suite of multimodal models useful for downstream ecological tasks covering six modalities: ground-level image, geographic location, satellite image, text, audio, and environmental features.
                </p>
              </td>
            </tr>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='psm_image'>
          <source src="images/task_final.jpg"></div>
          <img src='images/task_final.jpg' width=100%>
        </div> 
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2406.01917">
          <span class="papertitle">GOMAA-Geo: GOal Modality Agnostic Active Geo-localization
          </span>
        </a>
        <br>
        <a href="https://sites.wustl.edu/aayush/">Anindya Sarkar</a>*,
    <strong>Srikumar Sastry</strong>*,
        <a href="https://aleksispi.github.io/">Aleksis Pirinen</a>,
        <a href="https://mig-ai.github.io/person-zhangchongjie.html">Chonjie Zhang</a>,
        <a href="https://jacobsn.github.io/">Nathan Jacobs</a>,
        <a href="https://vorobeychik.com/">Yevgeniy Vorobeychik</a>
        <br>
        <em>NeurIPS</em>, 2024
        <br>
        <a href="https://github.com/mvrl/GOMAA-Geo">project page</a>
        /
        <a href="https://github.com/mvrl/GOMAA-Geo">github</a>
        /
        <a href="https://arxiv.org/abs/2406.01917">arXiv</a>
        <p></p>
        <p>
          We consider the task of active geo-localization (AGL) in which an agent uses a sequence of visual cues observed during aerial navigation to find a target specified through multiple possible modalities.
        </p>
      </td>
    </tr>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='psm_image'>
          <source src="images/Sentinel_zl_1_USA_maps.png"></div>
          <img src='images/Sentinel_zl_1_USA_maps.png' width=100%>
        </div> 
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://openreview.net/forum?id=qnW0LQXY5L">
          <span class="papertitle">PSM: Learning Probabilistic Embeddings for Multi-scale Zero-shot Soundscape Mapping</span>
        </a>
        <br>
        <a href="https://subash-khanal.github.io/">Subash Khanal</a>,
        <a href="https://ericx003.github.io/">Eric Xing</a>,
		<strong>Srikumar Sastry</strong>,
        <a href="https://sites.wustl.edu/aayush/">Aayush Dhakal</a>,
        <a href="">Zhexiao Xiong</a>,
        <a href="https://adealgis.wixsite.com/adeel-ahmad-geog">Adeel Ahmad</a>,
        <a href="https://jacobsn.github.io/">Nathan Jacobs</a>
        <br>
        <em>ACM Multimedia</em>, 2024
        <br>
        <a href="https://subash-khanal.github.io/PSM/index.html">project page</a>
        /
        <a href="https://github.com/mvrl/PSM">github</a>
        /
        <a href="https://arxiv.org/abs/2408.07050">arXiv</a>
        <p></p>
        <p>
          A soundscape is defined by the acoustic environment a person perceives at a location. In this work, we propose a framework for mapping soundscapes across the Earth.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='psm_image'>
          <source src="images/geobind-3eb4928812ba8923.jpg"></div>
          <img src='images/geobind-3eb4928812ba8923.jpg' width=100%>
        </div> 
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2404.11720">
          <span class="papertitle">GEOBIND: Binding Text, Image, and Audio through Satellite Images
          </span>
        </a>
        <br>
        <a href="https://sites.wustl.edu/aayush/">Aayush Dhakal</a>,
        <a href="https://subash-khanal.github.io/">Subash Khanal</a>,
		<strong>Srikumar Sastry</strong>,
        <a href="https://adealgis.wixsite.com/adeel-ahmad-geog">Adeel Ahmad</a>,
        <a href="https://jacobsn.github.io/">Nathan Jacobs</a>
        <br>
        <em>IGARSS</em>, 2024 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
        <br>
        <a href="">project page</a>
        /
        <a href="">github</a>
        /
        <a href="https://arxiv.org/abs/2404.11720">arXiv</a>
        <p></p>
        <p>
          In this work, we present a deep-learning model, GeoBind, that can infer about multiple modalities, specifically text, image, and audio, from satellite imagery of a location. To do this, we use satellite images as the binding element and contrastively align all other modalities to the satellite image data.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='psm_image'>
          <source src="images/teaser_v2-bf5e793c72870cd1-2048x725.jpg"></div>
          <img src='images/teaser_v2-bf5e793c72870cd1-2048x725.jpg' width=100%>
        </div> 
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://openaccess.thecvf.com/content/CVPR2024W/EarthVision/html/Sastry_GeoSynth_Contextually-Aware_High-Resolution_Satellite_Image_Synthesis_CVPRW_2024_paper.html">
          <span class="papertitle">GeoSynth: Contextually-Aware High-Resolution Satellite Image Synthesis
          </span>
        </a>
        <br>
        <strong>Srikumar Sastry</strong>,
        <a href="https://subash-khanal.github.io/">Subash Khanal</a>,
        <a href="https://sites.wustl.edu/aayush/">Aayush Dhakal</a>,	
        <a href="https://jacobsn.github.io/">Nathan Jacobs</a>
        <br>
        <em>Earthvision (CVPR)</em>, 2024
        <br>
        <a href="geosynth/index.html">project page</a>
        /
        <a href="https://github.com/mvrl/GeoSynth">github</a>
        /
        <a href="https://arxiv.org/abs/2404.06637">arXiv</a>
        /
        <a href="https://source.washu.edu/2024/07/artificial-intelligence-meets-cartography/">press</a>
        <p></p>
        <p>
          We present GeoSynth, a model for synthesizing satellite images with global style and image-driven layout control. The global style control is via textual prompts or geographic location. These enable the specification of scene semantics or regional appearance respectively, and can be used together.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='psm_image'>
          <source src="images/Sat2Cap-Maps-2048x1088.jpg"></div>
          <img src='images/Sat2Cap-Maps-2048x1088.jpg' width=100%>
        </div> 
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://openaccess.thecvf.com/content/CVPR2024W/EarthVision/html/Dhakal_Sat2Cap_Mapping_Fine-Grained_Textual_Descriptions_from_Satellite_Images_CVPRW_2024_paper.html">
          <span class="papertitle">Sat2Cap: Mapping Fine-Grained Textual Descriptions from Satellite Images
          </span>
        </a>
        <br>
        
        <a href="https://sites.wustl.edu/aayush/">Aayush Dhakal</a>,	
        <a href="https://adealgis.wixsite.com/adeel-ahmad-geog">Adeel Ahmad</a>,
        <a href="https://subash-khanal.github.io/">Subash Khanal</a>,
        <strong>Srikumar Sastry</strong>,
        <a href="https://hannah-rae.github.io/">Hannah Kerner</a>,
        <a href="https://jacobsn.github.io/">Nathan Jacobs</a>
        <br>
        <em>Earthvision (CVPR)</em>, 2024 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Award)</strong></font>
        <br>
        <a href="https://github.com/mvrl/Sat2Cap">project page</a>
        /
        <a href="https://github.com/mvrl/Sat2Cap">github</a>
        /
        <a href="https://arxiv.org/abs/2307.15904">arXiv</a>
        /
        <a href="https://source.washu.edu/2024/07/artificial-intelligence-meets-cartography/">press</a>
        <p></p>
        <p>
          We propose a weakly supervised approach for creating maps using free-form textual descriptions. We refer to this work of creating textual maps as zero-shot mapping.
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='psm_image'>
          <source src="images/climsatdiff.jpg"></div>
          <img src='images/climsatdiff.jpg' width=100%>
        </div> 
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="">
          <span class="papertitle">ClimSatDiff: Synthesizing the Earth's Surface Conditioned on Climatic Variables using Diffusion Models</span>
        </a>
        <br>
        <strong>Srikumar Sastry</strong>,
        <a href="https://adealgis.wixsite.com/adeel-ahmad-geog">Adeel Ahmad</a>,
        <a href="https://sites.wustl.edu/aayush/">Aayush Dhakal</a>,
        <a href="https://subash-khanal.github.io/">Subash Khanal</a>,
        <a href="https://ericx003.github.io/">Eric Xing</a>,
        <a href="https://jacobsn.github.io/">Nathan Jacobs</a>
        <br>
        <em>American Geophysical Union (AGU)</em>, 2024
        <br>
        <a href="https://subash-khanal.github.io/PSM/index.html">project page</a>
        <p></p>
        <p>
          In this work, we present a model that can synthesize the visual appearance of the Earth's surface conditioned on climate and time. We use PRISM and temporally rich cloud-free Sentinel-2 Level 2A and LandSat-8 for this study. 
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='psm_image'>
          <source src="images/vlpl.png"></div>
          <img src='images/vlpl.png' width=100%>
        </div> 
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://openaccess.thecvf.com/content/CVPR2024W/LIMIT/html/Xing_Vision-Language_Pseudo-Labels_for_Single-Positive_Multi-Label_Learning_CVPRW_2024_paper.html">
          <span class="papertitle">Vision-Language Pseudo-Labels for Single-Positive Multi-Label Learning
          </span>
        </a>
        <br>
        
        <a href="https://xtrigold.github.io/">Xin Xing</a>,	
        <a href="https://adealgis.wixsite.com/adeel-ahmad-geog">Zhexiao Zhang</a>,
        <a href="https://subash-khanal.github.io/">Abby Stylianou</a>,
        <strong>Srikumar Sastry</strong>,
        <a href="https://hannah-rae.github.io/">Liyu Gong</a>,
        <a href="https://jacobsn.github.io/">Nathan Jacobs</a>
        <br>
        <em>LIMIT (CVPR)</em>, 2024
        <br>
        <a href="https://github.com/mvrl/VLPL">project page</a>
        /
        <a href="https://github.com/mvrl/VLPL">github</a>
        /
        <a href="https://arxiv.org/abs/2310.15985">arXiv</a>
        <p></p>
        <p>
          We propose a novel model called Vision-Language Pseudo-Labeling (VLPL) which uses a vision-language model to suggest strong positive and negative pseudo-labels.
        </p>
      </td>
    </tr>
  
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='psm_image'>
          <source src="images/ldsdm.png"></div>
          <img src='images/ldsdm.png' width=100%>
        </div> 
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2312.08334">
          <span class="papertitle">LD-SDM: Language-Driven Hierarchical Species Distribution Modeling
          </span>
        </a>
        <br>
        
        <strong>Srikumar Sastry</strong>,
        <a href="https://xtrigold.github.io/">Xin Xing</a>,
        <a href="https://sites.wustl.edu/aayush/">Aayush Dhakal</a>,	
        <a href="https://subash-khanal.github.io/">Subash Khanal</a>,
        <a href="https://adealgis.wixsite.com/adeel-ahmad-geog">Adeel Ahmad</a>,
        <a href="https://jacobsn.github.io/">Nathan Jacobs</a>
        <br>
        <em>Preprint</em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2312.08334">arXiv</a>
        <p></p>
        <p>
          We focus on the problem of species distribution modeling using global-scale presence-only data. To capture a stronger implicit relationship between species, we encode the taxonomic hierarchy of species using a large language model.
        </p>
      </td>
    </tr>
  
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='psm_image'>
          <source src="images/birdsat.jpg"></div>
          <img src='images/birdsat.jpg' width=100%>
        </div> 
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://openaccess.thecvf.com/content/WACV2024/html/Sastry_BirdSAT_Cross-View_Contrastive_Masked_Autoencoders_for_Bird_Species_Classification_and_WACV_2024_paper.html">
          <span class="papertitle">BirdSAT: Cross-View Contrastive Masked Autoencoders for Bird Species Classification and Mapping
          </span>
        </a>
        <br>
        
        <strong>Srikumar Sastry</strong>,
        <a href="https://subash-khanal.github.io/">Subash Khanal</a>,
        <a href="https://sites.wustl.edu/aayush/">Aayush Dhakal</a>,
        <a href="">Di Huang</a>,
        <a href="https://jacobsn.github.io/">Nathan Jacobs</a>
        <br>
        <em>WACV</em>, 2024
        <br>
        <a href="birdsat/index.html">project page</a>
        /
        <a href="https://github.com/mvrl/BirdSAT#birdsat-cross-view-contrastive-masked-autoencoders-for-bird-species-classification-and-mapping">github</a>
        /
        <a href="https://arxiv.org/abs/2310.19168">arXiv</a>
        <p></p>
        <p>
          We propose a metadata-aware self-supervised learning (SSL) framework useful for fine-grained classification and ecological mapping of bird species around the world. Our framework unifies two SSL strategies: Contrastive Learning (CL) and Masked Image Modeling (MIM), while also enriching the embedding space with metadata available with ground-level imagery of birds.
        </p>
      </td>
    </tr>
  
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='psm_image'>
          <source src="images/geoclap.png"></div>
          <img src='images/geoclap.png' width=100%>
        </div> 
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://proceedings.bmvc2023.org/813/">
          <span class="papertitle">Learning Tri-modal Embeddings for Zero-Shot Soundscape Mapping
          </span>
        </a>
        <br>
        
        <a href="https://subash-khanal.github.io/">Subash Khanal</a>,
        <strong>Srikumar Sastry</strong>,
        <a href="https://sites.wustl.edu/aayush/">Aayush Dhakal</a>,
        <a href="https://jacobsn.github.io/">Nathan Jacobs</a>
        <br>
        <em>BMVC</em>, 2023
        <br>
        <a href="">project page</a>
        /
        <a href="https://github.com/mvrl/geoclap">github</a>
        /
        <a href="https://arxiv.org/abs/2309.10667">arXiv</a>
        <p></p>
        <p>
          We focus on the task of soundscape mapping, which involves predicting the most
probable sounds that could be perceived at a particular geographic location. We utilise
recent state-of-the-art models to encode geotagged audio, a textual description of the audio, and an overhead image of its capture location using contrastive pre-training. 
        </p>
      </td>
    </tr>
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='psm_image'>
          <source src="images/Task-Agnostic-Cost-Prediction.png"></div>
          <img src='images/Task-Agnostic-Cost-Prediction.png' width=100%>
        </div> 
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/document/10281434">
          <span class="papertitle">Task Agnostic Cost Prediction Module for Semantic Labeling in Active Learning
          </span>
        </a>
        <br>
        
        <strong>Srikumar Sastry</strong>,
        <a href="https://jacobsn.github.io/">Nathan Jacobs</a>,
        <a href="https://research.utwente.nl/en/persons/mariana-belgiu">Mariana Belgiu</a>,
        <a href="https://research.utwente.nl/en/persons/raian-vargas-maretto">Raian Vargas Maretto</a>
        <br>
        <em>IGARSS</em>, 2023 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
        <p></p>
        <p>
          We consider the problem of cost effective active learning for semantic segmentation, which aims at reducing the efforts of semantically annotating images. 
        </p>
      </td>
    </tr>

 </tbody></table>

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table id="reviewer" width="100%" align="center" border="0" cellpadding="20"><tbody>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><p align="center" style="margin-left:-2.5em;">Service</p><img src="images/mentor.jpg"></td>
              <td width="75%" valign="center">
                <a href="https://neurips.cc/">Reviewer, NeurIPS</a> [2024<font color="red"><strong> (Top Reviewer, Top 8%)</strong></font>, 2025]
                <br>
                <a href="https://jmlr.org/tmlr/">Reviewer, Transactions on Machine Learning Research (TMLR) </a>[2025]
                <br>
                <a href="https://cvpr.thecvf.com/">Reviewer, CVPR </a>[2025]
                <br>
                <a href="https://iccv.thecvf.com/">Reviewer, ICCV </a>[2025]
                <br>
                <a href="https://icml.cc/">Reviewer, ICML </a>[2025]
                <br>
                <a href="https://iclr.cc/">Reviewer, ICLR </a>[2025]
                <br>
                <a href="https://acmmm2025.org/">Reviewer, ACM Multimedia </a>[2025]
                <br>
                <a href="https://aistats.org/aistats2025/index.html">Reviewer, AISTATS </a>[2025]
                <br>
                <a href="https://eccv.ecva.net/">Reviewer, ECCV </a>[2024]
                <br>
                <a href="https://www.sciencedirect.com/journal/isprs-journal-of-photogrammetry-and-remote-sensing/about/call-for-papers#vision-language-models-for-remote-sensing-analysis-and-interpretation">Reviewer, ISPRS (VLM For RS) </a>[2024]
                <br>
                <a href="https://geoai.ornl.gov/cv4eo-wacv/">Reviewer, WACV (CV4EO) </a>[2024-2025]
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><p align="center" style="margin-left:-2.5em;">Teaching</p>
                <img src="images/peer_review.jpg" alt="cs188">
              </td>
              <td width="75%" valign="center">
                <a href="https://environment.wustl.edu/programs/summer-undergraduate-research/">Center for Environment Undergraduate Research Mentor, Summer 2024</a>
                <br>
                <a href="https://environment.wustl.edu/programs/summer-undergraduate-research/">Center for Environment Undergraduate Research Mentor, Summer 2023</a>
                <br>
                <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CSE 559A Spring 2023</a>
              </td>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><p align="center" style="margin-left:-2.5em;">Awesome Huggingface Demos</p>
                <img src="images/gen.jpg" alt="cs188">
              </td>
              <td width="75%" valign="center">
                 <a href="huggingface-demos/Awesome (Huggingface) Demos 54f8e8e50813400893669605812ae392.html">List of my favorite huggingface demos</a>
              </td>
            </tr>
            
            <!-- <tr>
              <td align="center" style="padding:20px;width:25%;vertical-align:middle">
                <h2>Basically <br> Blog Posts</h2>
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                <br>
                <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
                <br>
                <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                <br>
                <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>
              </td>
            </tr> -->
            
            
          </tbody></table>
          <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
                </p>
              </td>
            </tr>
          </tbody></table> -->
        </td>
      </tr>
    </table>
  </body>
</html>
